{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MEMIT test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input params\n",
    "script_dir = \"sbatches_101023/test_scripts\"\n",
    "results_dir = \"log_memit_101023_test_results\"\n",
    "# specify output path\n",
    "results_dump_json = \"memit_results.test.final.float16.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    # 'backpack-gpt2',\n",
    "    'pythia-70m',\n",
    "    'pythia-160m',\n",
    "    'pythia-410m',\n",
    "    'pythia-1b',\n",
    "    'pythia-1.4b',\n",
    "    'pythia-2.8b',\n",
    "    'pythia-6.9b'\n",
    "]\n",
    "dnames = [\n",
    "    'company', \n",
    "    'country', \n",
    "    'verbs', \n",
    "    'temporal', \n",
    "    'stereoset', \n",
    "    'gender'    \n",
    "]\n",
    "leagues = [1e-3, 1e-4, 1e-5]\n",
    "subject_types = ['true', 'prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = []\n",
    "for root, dirs, files in os.walk(script_dir):\n",
    "    for fname in files:\n",
    "        if 'noedit' not in fname:\n",
    "            fnames.append(fname)\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps = []\n",
    "for fname in fnames:\n",
    "    vals = fname[:-7].split('_')\n",
    "    exps.append(vals[0] + '__' + vals[1] +'_' + vals[2] + '__' + vals[3] )\n",
    "len(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test results\n",
    "test_results = defaultdict(list)\n",
    "for root, dirs, files in os.walk(results_dir):\n",
    "    for fname in files:\n",
    "        if 'noedit' in fname:\n",
    "            continue\n",
    "        vals = fname[:-5].split('__')\n",
    "        exp_id = '__'.join(vals[:-1])\n",
    "    \n",
    "        with open(os.path.join(results_dir, fname), 'r') as fh:\n",
    "            data = json.load(fh)\n",
    "            test_results[exp_id].append(data)\n",
    "\n",
    "for k in sorted(test_results.keys()):\n",
    "    if len(test_results[k]) != 5:\n",
    "        print(\"Warning: did not find 5 runs for\", k, len(test_results[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(exps):\n",
    "    assert k in test_results.keys()\n",
    "for k in test_results.keys():\n",
    "    assert k in exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_sweep import model_name_to_short\n",
    "def get_test_results(model_name, league, dname, subject_type, verbose=False):\n",
    "    exp_id = f'{model_name_to_short(model_name)}__{dname}-{subject_type}_subject__{league}'\n",
    "\n",
    "    # get rid of runs that are out-of-league\n",
    "    options = []\n",
    "    for exp_run in test_results[exp_id]:\n",
    "        league_cutoff = exp_run['noedit']['general_score'] * (1+league)\n",
    "        if exp_run['edit']['general_score'] < league_cutoff:\n",
    "            options.append(exp_run)\n",
    "    if verbose:\n",
    "        print(f\"{exp_id} has {len(test_results[exp_id])} entries, of which {len(options)} are in-league\")\n",
    "\n",
    "    general_scores = [exp_run['edit']['general_score'] for exp_run in options]\n",
    "    intervention_scores = [exp_run['edit']['intervention_score'] for exp_run in options]\n",
    "    hard_negative_scores = [exp_run['edit']['hard_negative_score'] for exp_run in options]\n",
    "\n",
    "    baseline_intervention = [exp_run['noedit']['intervention_score'] for exp_run in options]\n",
    "    baseline_hard_negative = [exp_run['noedit']['hard_negative_score'] for exp_run in options]\n",
    "\n",
    "    success_rate_change = np.array(baseline_intervention) - np.array(intervention_scores)\n",
    "    hard_negative_score_change = np.array(hard_negative_scores) - np.array(baseline_hard_negative)\n",
    "    return {\n",
    "        'intervention_score': {\n",
    "            'mean': np.mean(intervention_scores),\n",
    "            'stdv': np.std(intervention_scores),\n",
    "        },\n",
    "        'success_rate_change': {\n",
    "            'mean': np.mean(success_rate_change),\n",
    "            'stdv': np.std(success_rate_change),\n",
    "            # 'full_baseline_intervention': baseline_intervention,\n",
    "            # 'full_intervention_scores': intervention_scores,\n",
    "        },\n",
    "        'hard_negative_score': {\n",
    "            'mean': np.mean(hard_negative_scores),\n",
    "            'stdv': np.std(hard_negative_scores),\n",
    "        },\n",
    "        'hard_negative_score_change': {\n",
    "            'mean': np.mean(hard_negative_score_change),\n",
    "            'stdv': np.std(hard_negative_score_change),\n",
    "        },\n",
    "        'n': len(general_scores),\n",
    "        'out_of': len(test_results[exp_id])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/sachen/miniconda3/envs/backpacks-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/nlp/scr/sachen/miniconda3/envs/backpacks-env/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/nlp/scr/sachen/miniconda3/envs/backpacks-env/lib/python3.9/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/nlp/scr/sachen/miniconda3/envs/backpacks-env/lib/python3.9/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/nlp/scr/sachen/miniconda3/envs/backpacks-env/lib/python3.9/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for subject_type in subject_types:\n",
    "    results[subject_type] = {}\n",
    "    for model_name in model_names:\n",
    "        results[subject_type][model_name] = {}\n",
    "        for dname in dnames:\n",
    "            results[subject_type][model_name][dname] = {}\n",
    "            for league in leagues:\n",
    "                results[subject_type][model_name][dname][league] = get_test_results(model_name, league, dname, subject_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['oracle'] = results['true']\n",
    "del results['true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intervention_score': {'mean': 0.5666666666666667,\n",
       "  'stdv': 0.0072435582280029175},\n",
       " 'success_rate_change': {'mean': 0.3222222222222222,\n",
       "  'stdv': 0.0072435582280029175},\n",
       " 'hard_negative_score': {'mean': 1.6789426113696808,\n",
       "  'stdv': 0.00294959467269748},\n",
       " 'hard_negative_score_change': {'mean': 0.005612117686170137,\n",
       "  'stdv': 0.00294959467269748},\n",
       " 'n': 5,\n",
       " 'out_of': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek at results for a specific set up\n",
    "model_name = 'pythia-6.9b'\n",
    "dname = 'gender'\n",
    "league = 1e-5\n",
    "results['oracle'][model_name][dname][league]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dump_json, \"w\") as fh:\n",
    "    json.dump(results, fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backpacks-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
