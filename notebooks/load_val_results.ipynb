{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated MEMIT result agg (10-07-23 run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'log_memit_100723'\n",
    "\n",
    "LEAGUES = [0.001, 0.0001, 0.00001]\n",
    "dnames = [\n",
    "    'company', \n",
    "    'country', \n",
    "    'verbs', \n",
    "    'temporal', \n",
    "    'stereoset', \n",
    "    'gender'    \n",
    "]\n",
    "model_names = [\n",
    "    'backpack-gpt2',\n",
    "    'pythia-70m',\n",
    "    'pythia-160m',\n",
    "    'pythia-410m',\n",
    "    'pythia-1b',\n",
    "    'pythia-1.4b',\n",
    "    'pythia-2.8b',\n",
    "    'pythia-6.9b'\n",
    "]\n",
    "subject_types = [\n",
    "    'true_subject', 'prefix_subject'\n",
    "]\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for st in subject_types:\n",
    "    results_dict[st] = {}\n",
    "    for m in model_names:\n",
    "        results_dict[st][m] = {}\n",
    "        for d in dnames:\n",
    "            results_dict[st][m][d] = {}\n",
    "for root, _, files in os.walk(result_dir):\n",
    "    for fname in files:\n",
    "        if 'noedit' in fname:\n",
    "            continue # skip no-edit \n",
    "\n",
    "        param_keys = ['model', 'dataset', 'layers', 'v_num_grad_steps', 'clamp_norm_factor', \n",
    "                  'mom2_update_weight', 'kl_factor']\n",
    "        param_dict = dict(zip(param_keys, fname[:-5].split('__')))\n",
    "\n",
    "        param_str = '__'.join(param_dict[x] for x in param_keys[4:])\n",
    "        dname = param_dict['dataset'].split('-')[0]\n",
    "        subject_type = param_dict['dataset'].split('-')[1]\n",
    "        results_dict[subject_type][param_dict['model']][dname][param_str] = json.load(open(os.path.join(root, fname), 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite noedit results for prefix_subject with true_subject\n",
    "for m in model_names:\n",
    "    for d in dnames:\n",
    "        for param_str in results_dict['prefix_subject'][m][d].keys():\n",
    "            results_dict['prefix_subject'][m][d][param_str]['noedit'] = results_dict['true_subject'][m][d][param_str]['noedit']\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether results fall in each league\n",
    "for st in subject_types:\n",
    "    for m in model_names:\n",
    "        for d in dnames:\n",
    "            for param_str in results_dict[st][m][d].keys():\n",
    "                data = results_dict[st][m][d][param_str] \n",
    "\n",
    "                for league in LEAGUES:\n",
    "                    league_loss_cutoff = data['noedit']['general_score']*(1+league)\n",
    "                    data['edit']['in_league_{}'.format(league)] = data['edit']['general_score'] < league_loss_cutoff\n",
    "                data['edit']['intervention_score_delta'] = data['edit']['intervention_score'] - data['noedit']['intervention_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best_config for each league\n",
    "best_configs = {}\n",
    "for m in model_names:\n",
    "    best_configs[m] = {}\n",
    "    for d in dnames:\n",
    "        for st in subject_types:\n",
    "            full_dname = '{}-{}'.format(d, st)\n",
    "            best_configs[m][full_dname] = {}\n",
    "best_results = {}\n",
    "for st in subject_types:\n",
    "    best_results[st] = {}\n",
    "    for m in model_names:\n",
    "        best_results[st][m] = {}\n",
    "        for d in dnames:\n",
    "            best_results[st][m][d] = {}\n",
    "\n",
    "for st in subject_types:\n",
    "    for m in model_names:\n",
    "        for d in dnames:\n",
    "            full_dname = '{}-{}'.format(d, st)\n",
    "            for league in LEAGUES:\n",
    "                # find the best config for each league\n",
    "\n",
    "                # find the runs in each league\n",
    "                options = []\n",
    "                for param_str in results_dict[st][m][d].keys():\n",
    "                    data = results_dict[st][m][d][param_str] \n",
    "                    if data['edit']['in_league_{}'.format(league)]:\n",
    "                        options.append(data)\n",
    "                # find the best run\n",
    "                lowest_intervention_score_delta = float('inf')\n",
    "                best_config = None \n",
    "                best_data = None\n",
    "                for data in options:\n",
    "                    if data['edit']['intervention_score_delta'] < lowest_intervention_score_delta:\n",
    "                        lowest_intervention_score_delta = data['edit']['intervention_score_delta']\n",
    "                        best_config = data['edit']['override_params']\n",
    "                        best_data = data\n",
    "\n",
    "                best_configs[m][full_dname][league] = best_config\n",
    "                best_results[st][m][d][league] = best_data\n",
    "\n",
    "                # # print\n",
    "                score_deltas = [data['edit']['intervention_score_delta'] for data in options]\n",
    "                if len(options) == 0:\n",
    "                #     print(\"NO OPTIONS\")\n",
    "                    continue \n",
    "                best_index = score_deltas.index(min(score_deltas))\n",
    "\n",
    "                scores = [data['edit']['intervention_score'] for data in options]\n",
    "                best_score_index = scores.index(min(scores))\n",
    "                assert best_score_index == best_index\n",
    "\n",
    "                # print('score_deltas:', score_deltas)\n",
    "                # print('best_index:', best_index)\n",
    "                # print('best_score:', score_deltas[best_index])\n",
    "\n",
    "                # print('chosen config:', options[best_index])\n",
    "                # print('chosen config:', best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results.keys()\n",
    "best_results['oracle'] = best_results['true_subject']\n",
    "best_results['prefix'] = best_results['prefix_subject']\n",
    "del best_results['true_subject']\n",
    "del best_results['prefix_subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noedit': {'intervention_score': 0.7989690721649485,\n",
       "  'general_score': 2.643992486733253,\n",
       "  'rest_of_prompt_score': 2.6830669109093,\n",
       "  'hard_negative_score': None},\n",
       " 'edit': {'intervention_score': 0.7783505154639175,\n",
       "  'general_score': 2.644253354275785,\n",
       "  'rest_of_prompt_score': 2.3759197688479055,\n",
       "  'hard_negative_score': None,\n",
       "  'override_params': {'layers': None,\n",
       "   'v_num_grad_steps': 20,\n",
       "   'clamp_norm_factor': 0.6828108071671678,\n",
       "   'mom2_update_weight': 62225,\n",
       "   'kl_factor': 0.07505701446082196},\n",
       "  'in_league_0.001': True,\n",
       "  'in_league_0.0001': True,\n",
       "  'in_league_1e-05': False,\n",
       "  'intervention_score_delta': -0.020618556701030966}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results['prefix']['pythia-1.4b']['country'][0.001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"memit_results.val.final.json\", \"w\") as fh:\n",
    "    json.dump(best_results, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test scripts (using best val config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_test_scripts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5\n",
    "out_log_dir = 'log_memit_100723_test_results'\n",
    "sweep_script_dir = 'sbatches_100723'\n",
    "sweep_script_write_dir = 'sbatches_100723/test_scripts'\n",
    "\n",
    "from make_sweep import get_sbatch_header, model_name_to_short, model_name_to_full\n",
    "# model_to_queue, model_to_jags\n",
    "\n",
    "\n",
    "def model_to_queue(model_name):\n",
    "    if '410m' in model_name or '160m' in model_name or '70m' in model_name or 'backpack' in model_name:\n",
    "        return 'jag-standard'\n",
    "    else:\n",
    "        return 'jag-lo'\n",
    "    \n",
    "def model_to_jags(model_name):\n",
    "    # if '6.9b' in model_name or '2.8b' in model_name or '1b' in model_name or 'gpt-j' in model_name or '160m' in model_name:\n",
    "    #     return ['jagupard37', 'jagupard38', 'jagupard39']\n",
    "    # elif '1.4b' in model_name:\n",
    "    #     return ['jagupard32', 'jagupard33', 'jagupard34', 'jagupard35', 'jagupard36']\n",
    "    # elif '410m' in model_name:\n",
    "    #     return ['jagupard30', 'jagupard31', ]\n",
    "    # elif '70m' in model_name or 'backpack' in model_name:\n",
    "    #     return ['jagupard28', 'jagupard29', ]\n",
    "    # else:\n",
    "    #     raise ValueError\n",
    "    if '6.9b' in model_name or '2.8b' in model_name or '1.4b' in model_name or '1b' in model_name \\\n",
    "        or 'gpt-j' in model_name:\n",
    "        return ['jagupard37', 'jagupard38', 'jagupard39']\n",
    "    elif '410m' in model_name or '160m' in model_name or '70m' in model_name or 'backpack' in model_name:\n",
    "        return ['jagupard32', 'jagupard33', 'jagupard34', 'jagupard35', 'jagupard36']\n",
    "    else:\n",
    "        raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make scripts for the run on the test data\n",
    "\n",
    "if make_test_scripts:\n",
    "    machine_choosing_index = 0\n",
    "\n",
    "    dname_cfg_map = {\n",
    "        'company': 'company_ceo', 'country': 'country_capital', 'verbs': 'verb_conjugation', \n",
    "        'temporal': 'temporal', 'stereoset': 'stereoset', 'gender': 'pronoun_gender_bias'\n",
    "    }\n",
    "    run_cmds = []\n",
    "    filenames = []\n",
    "    for model_name in best_configs:\n",
    "        for full_dname in best_configs[model_name]:\n",
    "            for league in best_configs[model_name][full_dname]:\n",
    "                cur_config = best_configs[model_name][full_dname][league]\n",
    "                if cur_config is None:\n",
    "                    print(\">> WARNING: NO CONFIG YIELDED FOR\", model_name, full_dname, league)\n",
    "                    continue\n",
    "                dname, subject_type = full_dname.split('-')\n",
    "\n",
    "\n",
    "                short_model_name = model_name_to_short(model_name)\n",
    "                jag_options = model_to_jags(model_name)\n",
    "                nodelist = jag_options[machine_choosing_index % len(jag_options)]\n",
    "                machine_choosing_index += 1 \n",
    "\n",
    "\n",
    "                with open(f\"{sweep_script_write_dir}/{short_model_name}_{full_dname}_{league}.sbatch\", \"w\") as fh:\n",
    "                    filenames.append(f\"{sweep_script_write_dir}/{short_model_name}_{full_dname}_{league}.sbatch\")\n",
    "\n",
    "                    print(\n",
    "                        get_sbatch_header(\n",
    "                            run_name=f'{short_model_name}_{dname[:3]}_test-sweep', \n",
    "                            partition=model_to_queue(model_name), \n",
    "                            nodelist=nodelist,\n",
    "                            log_output_dir=f\"{sweep_script_dir}/test_logs\",\n",
    "                            num_hrs=12,\n",
    "                        ),\n",
    "                        file=fh\n",
    "                    )\n",
    "\n",
    "                    for t in range(num_trials):\n",
    "                        test_command = (\n",
    "                            f'python3 run_memit.py \"{model_name_to_full[model_name]}\" --v_num_grad_steps 20 '\n",
    "                            f'--clamp_norm_factor {cur_config[\"clamp_norm_factor\"]} '\n",
    "                            f'--mom2_update_weight {cur_config[\"mom2_update_weight\"]} '\n",
    "                            f'--kl_factor {cur_config[\"kl_factor\"]} '\n",
    "                            f'--dataset_names {dname} '\n",
    "                            f'--subject_types {subject_type} '\n",
    "                            f'--log_dir {out_log_dir} --test_mode '\n",
    "                            f'--override_exp_name {short_model_name}__{full_dname}__{league}__trial{t} '\n",
    "                            f'--seed {t}')\n",
    "\n",
    "                        run_cmd = (\n",
    "                            f\"{test_command} >> {sweep_script_dir}/test_logs/log.{short_model_name}_{full_dname}_{league}_{t}.txt\"\n",
    "                        )\n",
    "                        # run_cmd = (\n",
    "                        #     f\"srun --unbuffered run_as_child_processes '{test_command}' \"\n",
    "                        #     f\">> {sweep_script_dir}/test_logs/log.{model_name}_{full_dname}_{league}_{t}.txt\"\n",
    "                        # )                    \n",
    "                        print(run_cmd, file=fh)\n",
    "                        run_cmds.append(run_cmd)\n",
    "\n",
    "\n",
    "    for x in filenames:\n",
    "        print('sbatch', x)\n",
    "    print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backpacks-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
